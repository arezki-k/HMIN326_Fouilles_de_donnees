{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TP3vnRyEzip"
   },
   "source": [
    "# Classification of movies opinions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbP7jlyjEzit"
   },
   "source": [
    "# Pre-traitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group AA\n",
    "- KACIOUI Arezki\n",
    "- KHEFFACHE Cherif\n",
    "- SHIRALI POUR Amir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfvK9JQxEzit"
   },
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "10XQ2NziEziu"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "import warnings\n",
    "import unicodedata\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from unidecode import unidecode\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One time installed tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install contractions\n",
    "# pip install treetaggerwrapper\n",
    "# pip install unidecode\n",
    "# pip install treetaggerwrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptionStopWords = ['no', 'not', 'nor', 'down', 'up', 'on', 'too', 'out']\n",
    "newStopWords = set(stopwords.words('english')).difference(exceptionStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieComments = pandas.read_csv('data/dataset.csv', sep = '\\t', header = None, encoding = \"utf8\")\n",
    "movieComments['lables'] = pandas.read_csv('data/labels.csv', sep = '\\t', header = None, encoding = \"utf8\")\n",
    "movieComments.columns = ['comments','lables']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW1LGC_xEziu"
   },
   "source": [
    "Pre-traitements:\n",
    "\n",
    "1- Remove non-ASCII characters\n",
    "\n",
    "2- Remove contractions\n",
    "\n",
    "3- To lowercase\n",
    "\n",
    "4- Remove ponctuations\n",
    "\n",
    "5- Remove stopwords\n",
    "\n",
    "6- Remove numbers\n",
    "\n",
    "7- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove special caracters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialCaracters(movieComments):\n",
    "    for index, comment in movieComments.iterrows():\n",
    "        comment = comment['comments']\n",
    "        result = re.sub('[^a-zA-Z\\n\\.]', ' ', comment)\n",
    "        comment = re.sub(' +', ' ', result)\n",
    "        # Removing non ASCII characters\n",
    "        comment = unicodedata.normalize('NFKD', comment).encode(\"ascii\", \"ignore\").decode(\"utf-8\", 'ignore')\n",
    "        movieComments.loc[index, 'comments'] = comment    \n",
    "    return movieComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieComments = removeSpecialCaracters(movieComments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeContractions(movieComments):\n",
    "    for index, comment in movieComments.iterrows():\n",
    "        comment = comment['comments']\n",
    "        comment = contractions.fix(comment, slang = True)\n",
    "        movieComments.loc[index, 'comments'] = comment \n",
    "    return movieComments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieComments = removeContractions(movieComments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(movieComments):\n",
    "    for index, comment in movieComments.iterrows():\n",
    "        comment = comment['comments']\n",
    "        removedStopWords =  [word for word in comment.split() if word.lower() not in newStopWords]\n",
    "        comment = \"\".join([\" \" + i for i in removedStopWords])\n",
    "        movieComments.loc[index, 'comments'] = comment \n",
    "    return movieComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieComments = removeStopWords(movieComments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normilizeComments(movieComments):\n",
    "    for index, comment in movieComments.iterrows():\n",
    "        comment = comment['comments']\n",
    "        comment = [word.lower() for word in comment.split()]\n",
    "        comment = \" \".join(comment)\n",
    "        movieComments.loc[index, 'comments'] = comment \n",
    "    return movieComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieComments = normilizeComments(movieComments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizedText(movieComments):\n",
    "    for index, comment in movieComments.iterrows():\n",
    "        comment = comment['comments']\n",
    "        # Tokenization\n",
    "        tokenizedText = word_tokenize(comment)        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        comment = [lemmatizer.lemmatize(word, pos = 'v') for word in tokenizedText]\n",
    "        comment = \" \".join([\" \" + i for i in comment])\n",
    "        movieComments.loc[index, 'comments'] = comment \n",
    "    return movieComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieComments = tokenizedText(movieComments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to arff extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install arff\n",
    "\n",
    "import arff\n",
    "arff.dump('dataset_tokenized_lemmatized.arff'\n",
    "      , movieComments.values\n",
    "      , relation='movieComments'\n",
    "      , names=movieComments.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpRY__DeEziv"
   },
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLHamVU_Eziv"
   },
   "source": [
    "Vectorization with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization_full_text(movieComments):\n",
    "    full_text = movieComments['comments'].astype(str).tolist()\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(full_text)\n",
    "    vectors = vectorizer.transform(full_text)\n",
    "    return vectorizer, vectors, full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, vectors, full_text = vectorization_full_text(movieComments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in vectorizer.vocabulary_.items():\n",
    "    vectorizer.vocabulary_[k] = int(v)\n",
    "    \n",
    "with open('vectorizer.vocab_.json', 'w') as output:\n",
    "    json.dump(vectorizer.vocabulary_, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective realize the classifications with scikit-learn"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "movieanalyser.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
